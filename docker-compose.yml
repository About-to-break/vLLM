services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ${HF_CACHE_DIR:-~/.cache/huggingface}:/root/.cache/huggingface
    command: [
      "--host", "0.0.0.0",
      "--port", "8000",
      "--model", "${VLLM_MODEL}",
      "--quantization", "${VLLM_QUANTIZATION}",
      "--max-model-len", "${VLLM_MAX_MODEL_LEN}",
      "--gpu-memory-utilization", "${VLLM_GPU_MEMORY_UTILIZATION}",
      "--hf-token", "${HF_TOKEN}",
      "--kv-cache-dtype","${KV_CACHE_DTYPE}",
      "--swap-space","${VLLM_RAM_SWAP_SPACE}",
      "--max-num-batched-tokens","${VLLM_MAX_BATCHED_TOKENS}",
      "--trust-remote-code",
      "--enable-prefix-caching",
      "--log-level debug",
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://127.0.0.1:8000/health || exit 1" ]
      interval: 60s
      timeout: 3s
      retries: 6
      start_period: 60s
