# BE SURE TO ADD THIS TO .gitignore on PROD
# --- --- --- --- --- --- --- --- --- --- ---

# Your hugging face token used to download data directly
HF_TOKEN=...

# Model name
VLLM_MODEL=Qwen/Qwen3-14B-AWQ

# Quantisation for quantised models
VLLM_QUANTIZATION=awq_marlin

# Max active tokens
VLLM_MAX_MODEL_LEN=4196

# Percent of used (reserved) GPU memory
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Don't touch if you don't know what you are doing
KV_CACHE_DTYPE=auto

# Amount of RAM allowed to be used as extra kv cache space
VLLM_RAM_SWAP_SPACE=8

# Max batch
VLLM_MAX_BATCHED_TOKENS=2048